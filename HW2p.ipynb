{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import math\r\n",
    "from scipy.special import expit"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rg2uSiFSRJe"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 1\n",
    "## Dataset Generation"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "oDTSO7-MSRJj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write a function to **generate a training set** of size $m$\n",
    "- randomly generate a weight vector $w \\in \\mathbb{R}^{10}$, normalize length\n",
    "- generate a training set $\\{(x_i , y_i)\\}$ of size m\n",
    "  - $x_i$: random vector in $\\mathbb{R}^{10}$ from $\\textbf{N}(0, I)$\n",
    "  - $y_i$: $\\{0, +1\\}$ with $P[y = +1] = \\sigma(w \\cdot x_i)$ and $P[y = 0] = 1 - \\sigma(w \\cdot x_i)$"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "Tx7KW28cSRJk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "source": [
    "def sigmoid(x):\r\n",
    "    return 1/(1+np.exp(-x))\r\n",
    "     \r\n",
    "def generate_vector(normalize=False):\r\n",
    "    v = np.random.randn(10)\r\n",
    "    if normalize:\r\n",
    "        sum = 0\r\n",
    "        for i in v:\r\n",
    "            sum += i * i \r\n",
    "        norm = math.sqrt(sum)\r\n",
    "        v = [i*1/norm for i in v]\r\n",
    "    return v\r\n",
    "\r\n",
    "def generate_label(x_i,w):\r\n",
    "    s = expit(np.dot(w,x_i))\r\n",
    "    X = np.random.uniform(low=0,high=1)\r\n",
    "    y_i = 0\r\n",
    "    if X <= s:\r\n",
    "        y_i = 1\r\n",
    "    return y_i\r\n",
    "\r\n",
    "def generate_data(m):\r\n",
    "    x = []\r\n",
    "    for i in range(m):\r\n",
    "        x.append(generate_vector(normalize=False))\r\n",
    "    x = np.array(x)\r\n",
    "    return x\r\n",
    "\r\n",
    "def new_data(m):\r\n",
    "    w = generate_vector(normalize=True) \r\n",
    "    x = generate_data(m)\r\n",
    "    y =[generate_label(i,w) for i in x ]\r\n",
    "    return w, x, y \r\n",
    "\r\n",
    "# w_ = generate_vector(normalize=True) \r\n",
    "# x_m = generate_data(10)\r\n",
    "# y_m = np.array([generate_label(i,w_) for i in x_m ])\r\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUHuXrsgSRJl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm 1: logistic regression\n",
    "\n",
    "The goal is to learn $w$.  Algorithm 1 is logistic\n",
    "  regression (you may use the built-in method LogisticRegression for this. Use max_iter=1000)."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "wx2-15fASRJy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "source": [
    "from sklearn.linear_model import LogisticRegression\r\n",
    "\r\n",
    "def algorithm1(X,Y):\r\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000).fit(X, Y)\r\n",
    "    w_prime = np.array(clf.coef_[0])\r\n",
    "    return w_prime"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vj8b21jgSRJz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm 2: gradient descent with square loss\n",
    "\n",
    "Define square loss as\n",
    "$$L_i(w^{(t)}) = \\frac{1}{2} \\left( \\sigma(w^{(t)} \\cdot x) - y_i \\right)^2$$\n",
    "\n",
    "  Algorithm 2 is\n",
    "  gradient descent with respect to square loss (code this\n",
    "  up yourself -- run for 1000 iterations, use step size eta = 0.01)."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "YzmNdy6ZSRJ3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "source": [
    "w_t = np.zeros(10)\r\n",
    "w_star, x , y = new_data(10)\r\n",
    "\r\n",
    "for j in range(10):\r\n",
    "    for i in range(10): \r\n",
    "        f = expit(np.dot(w_t, x[i]))\r\n",
    "        loss = 0.5*(f - y[i])**2\r\n",
    "        w_t -= 0.01*loss\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5\n",
      "0.5024054926971206\n",
      "-0.5040960531356005\n",
      "0.4997268957252963\n",
      "0.5013071712056889\n",
      "0.49909601917050767\n",
      "0.4948656958934071\n",
      "-0.5115727408172213\n",
      "0.48250116491129064\n",
      "0.5087782417307009\n",
      "0.48875049740641047\n",
      "0.5263764131541385\n",
      "-0.5246091448460825\n",
      "0.4988101205270909\n",
      "0.5046048770115346\n",
      "0.49726878820790554\n",
      "0.48621255736563185\n",
      "-0.528274581520767\n",
      "0.4604124228516663\n",
      "0.5186523703730174\n",
      "0.47736258760156375\n",
      "0.5505457792841946\n",
      "-0.5453217826587012\n",
      "0.49788037061654655\n",
      "0.5079488749861033\n",
      "0.4954157387453892\n",
      "0.4774452324292859\n",
      "-0.5451496147691148\n",
      "0.438163056537869\n",
      "0.5286585351882065\n",
      "0.4658284515734924\n",
      "0.5748370901265436\n",
      "-0.5661943148820727\n",
      "0.49693604062082203\n",
      "0.5113446394801678\n",
      "0.49353371227441145\n",
      "0.4685544063729437\n",
      "-0.5621851530347122\n",
      "0.4158110030687997\n",
      "0.5388055381548867\n",
      "0.4541413702229719\n",
      "0.5991665663426531\n",
      "-0.5871814666146378\n",
      "0.49597553383983356\n",
      "0.5147975824063877\n",
      "0.4916195718561377\n",
      "0.4595314250605883\n",
      "-0.5793649167739143\n",
      "0.39342057762507265\n",
      "0.5491011543580353\n",
      "0.442296119203083\n",
      "0.6234431824949219\n",
      "-0.6082318307174079\n",
      "0.4949972796821765\n",
      "0.5183129844991721\n",
      "0.48967023772436685\n",
      "0.4503685571407329\n",
      "-0.5966684248231158\n",
      "0.37106265925587145\n",
      "0.559551793882848\n",
      "0.4302893905117783\n",
      "0.6475690241170933\n",
      "-0.629287524634297\n",
      "0.4939997532453879\n",
      "0.5218959186031963\n",
      "0.4876827272041228\n",
      "0.44105927637419257\n",
      "-0.6140704335858098\n",
      "0.3488146118310571\n",
      "0.5701621431724198\n",
      "0.4181202308644621\n",
      "0.671440043743936\n",
      "-0.6502840788143682\n",
      "0.49298149677671405\n",
      "0.525551166057549\n",
      "0.485654198405177\n",
      "0.43159855809881364\n",
      "-0.6315404657003938\n",
      "0.3267598725413659\n",
      "0.5809347953829158\n",
      "0.4057904824671613\n",
      "0.6949472758337699\n",
      "-0.6711506225931492\n",
      "0.4919411425490499\n",
      "0.5292831280081601\n",
      "0.48358199674259733\n",
      "0.4219831810970828\n",
      "-0.6490424743868983\n",
      "0.304987150108721\n",
      "0.5918698829524157\n",
      "0.39330520718464074\n",
      "0.7179785448122419\n",
      "-0.691810426520674\n",
      "0.4908774364692684\n",
      "0.5330957343037793\n",
      "0.48146370289745155\n",
      "0.41221202331008244\n",
      "-0.6665346924123734\n",
      "0.28358919419353446\n",
      "0.6029647292194048\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm 3: stochastic gradient descent with square loss\n",
    "Similar to gradient descent, except we use the gradient at a single random training point every iteration."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "\n",
    "Measure error $\\|w - \\hat{w}\\|_2$ for each method at different sample size. For any\n",
    "  fixed value of $m$, choose many different $w$'s and average the\n",
    "  values $\\|w - \n",
    "  \\hat{w}\\|_2$ for Algorithms 1, 2 and 3.  Plot the results\n",
    "  for for each algorithm as you make $m$ large (use $m=50, 100, 150, 200, 250$).\n",
    "  Also record, for each algorithm, the time taken to run the overall experiment."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "5A-dLi3TSRJ-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "source": [
    "a1 = []\r\n",
    "\r\n",
    "for i in range(w_norms_a1.size):\r\n",
    "    M = [50,100,150,200,250]\r\n",
    "    w_norms_a1 = np.zeros(10)\r\n",
    "    for m in M:\r\n",
    "        w_star, x , y = new_data(m)\r\n",
    "        w_new = algorithm1(x,y)\r\n",
    "        w_sub = np.subtract(w_star,w_new)\r\n",
    "        w_norm_diff = np.linalg.norm(w_sub)\r\n",
    "        w_norms_a1[i] = w_norm_diff\r\n",
    "    print(np.average(w_norms_a1))\r\n",
    "    a1.append(np.average(w_norms_a1))\r\n",
    "\r\n",
    "    \r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.06439645630379462\n",
      "0.0531074375162686\n",
      "0.0521224498583167\n",
      "0.030675101640195924\n",
      "0.03492726280937618\n",
      "0.046498243791706376\n",
      "0.04100560678755705\n",
      "0.03073333286844171\n",
      "0.046051028181084294\n",
      "0.05682052170392674\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "from sklearn import datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "cancer = datasets.load_breast_cancer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each depth in $1, \\dots, 5$, instantiate an AdaBoost classifier with the base learner set to be a decision tree of that depth (set `n_estimators=10` and `learning_rate=1`), and then record the 10-fold cross-validated error on the entire breast cancer data set. Plot the resulting curve of accuracy against base classifier depth. Use $101$ as your random state for both the base learner as well as the AdaBoost classifier every time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw2_programming_sol.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "a549b6e126d5f0fb8e948efef61e129ac23a869a935398aeb4639a233ee46c64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}