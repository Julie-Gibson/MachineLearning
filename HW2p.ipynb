{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.special import expit\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rg2uSiFSRJe"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 1\n",
    "## Dataset Generation"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "oDTSO7-MSRJj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write a function to **generate a training set** of size $m$\n",
    "- randomly generate a weight vector $w \\in \\mathbb{R}^{10}$, normalize length\n",
    "- generate a training set $\\{(x_i , y_i)\\}$ of size m\n",
    "  - $x_i$: random vector in $\\mathbb{R}^{10}$ from $\\textbf{N}(0, I)$\n",
    "  - $y_i$: $\\{0, +1\\}$ with $P[y = +1] = \\sigma(w \\cdot x_i)$ and $P[y = 0] = 1 - \\sigma(w \\cdot x_i)$"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "Tx7KW28cSRJk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "def generate_vector(normalize=False):\n",
    "    v = np.random.randn(10)\n",
    "    if normalize:\n",
    "        sum = 0\n",
    "        for i in v:\n",
    "            sum += i * i \n",
    "        norm = math.sqrt(sum)\n",
    "        v = [i*1/norm for i in v]\n",
    "    return v\n",
    "\n",
    "def generate_label(x_i,w):\n",
    "    s = expit(np.dot(w,x_i))\n",
    "    X = np.random.uniform(low=0,high=1)\n",
    "    y_i = 0\n",
    "    if X <= s:\n",
    "        y_i = 1\n",
    "    return y_i\n",
    "\n",
    "def generate_data(m):\n",
    "    x = []\n",
    "    for i in range(m):\n",
    "        x.append(generate_vector(normalize=False))\n",
    "    x = np.array(x)\n",
    "    return x\n",
    "\n",
    "def new_data(m):\n",
    "    w = generate_vector(normalize=True) \n",
    "    x = generate_data(m)\n",
    "    y =[generate_label(i,w) for i in x ]\n",
    "    return w, x, y \n",
    "\n",
    "# w_ = generate_vector(normalize=True) \n",
    "# x_m = generate_data(10)\n",
    "# y_m = np.array([generate_label(i,w_) for i in x_m ])\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUHuXrsgSRJl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm 1: logistic regression\n",
    "\n",
    "The goal is to learn $w$.  Algorithm 1 is logistic\n",
    "  regression (you may use the built-in method LogisticRegression for this. Use max_iter=1000)."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "wx2-15fASRJy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def algorithm1(X,Y):\n",
    "    start = time.time()\n",
    "    clf = LogisticRegression(random_state=0, max_iter=1000).fit(X, Y)\n",
    "    w_prime = np.array(clf.coef_[0])\n",
    "    print(time.time()-start)\n",
    "    return w_prime"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vj8b21jgSRJz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm 2: gradient descent with square loss\n",
    "\n",
    "Define square loss as\n",
    "$$L_i(w^{(t)}) = \\frac{1}{2} \\left( \\sigma(w^{(t)} \\cdot x) - y_i \\right)^2$$\n",
    "\n",
    "  Algorithm 2 is\n",
    "  gradient descent with respect to square loss (code this\n",
    "  up yourself -- run for 1000 iterations, use step size eta = 0.01)."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "YzmNdy6ZSRJ3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "def loss_gradient(x,y,w):\n",
    "    sig = expit(np.dot(w,x))\n",
    "    return (sig - y)*sig*(1-sig)*x\n",
    "\n",
    "def gradient(x,y,w):\n",
    "    values = []\n",
    "    for i in range(len(x)):\n",
    "        values.append(loss_gradient(x[i],y[i],w))\n",
    "    return sum(values)\n",
    "\n",
    "def algorithm2(x,y):\n",
    "    start = time.time()\n",
    "    w_t = np.zeros(10)\n",
    "    for iterations in range(1000):\n",
    "        w_t -= 0.01*gradient(x,y,w_t)\n",
    "    print(time.time()-start)\n",
    "    return w_t"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm 3: stochastic gradient descent with square loss\n",
    "Similar to gradient descent, except we use the gradient at a single random training point every iteration."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "def stocastic_gradient(x,y,w):\n",
    "    random = np.random.randint(0,len(x))\n",
    "    return loss_gradient(x[random],y[random],w)\n",
    "\n",
    "def algorithm3(x,y):\n",
    "    start = time.time()\n",
    "    w_t = np.zeros(10)\n",
    "    for iterations in range(1000):\n",
    "        w_t -= 0.01*stocastic_gradient(x,y,w_t)\n",
    "    print(time.time()-start)\n",
    "    return w_t"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "\n",
    "Measure error $\\|w - \\hat{w}\\|_2$ for each method at different sample size. For any\n",
    "  fixed value of $m$, choose many different $w$'s and average the\n",
    "  values $\\|w - \n",
    "  \\hat{w}\\|_2$ for Algorithms 1, 2 and 3.  Plot the results\n",
    "  for for each algorithm as you make $m$ large (use $m=50, 100, 150, 200, 250$).\n",
    "  Also record, for each algorithm, the time taken to run the overall experiment."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "5A-dLi3TSRJ-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "\n",
    "a1 = []\n",
    "a2 = []\n",
    "a3 = []\n",
    "M = [50,100,150,200,250]\n",
    "\n",
    "# 10 repetitions of each algorithm \n",
    "for i in range(10):\n",
    "    for m in M:\n",
    "        w_star, x , y = new_data(m)\n",
    "        \n",
    "        w_new = algorithm1(x,y)\n",
    "        w_sub = w_star-w_new\n",
    "        w_norm_diff = np.linalg.norm(w_sub)\n",
    "        a1.append(w_norm_diff)\n",
    "\n",
    "        w_new_2 = algorithm2(x,y)\n",
    "        w_sub_2 = np.subtract(w_star,w_new_2)\n",
    "        w_norm_diff_2 = np.linalg.norm(w_sub_2)\n",
    "        a2.append(w_norm_diff_2)\n",
    "\n",
    "        w_new_3 = algorithm3(x,y) \n",
    "        w_sub_3 = np.subtract(w_star,w_new_3)\n",
    "        w_norm_diff_3 = np.linalg.norm(w_sub_3)\n",
    "        a3.append(w_norm_diff_3)\n",
    "\n",
    "print(np.average(a1))\n",
    "print(np.average(a2))\n",
    "print(np.average(a3))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6758945486549706\n",
      "0.9712871550758766\n",
      "0.6843225233208823\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "from sklearn import datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "cancer = datasets.load_breast_cancer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each depth in $1, \\dots, 5$, instantiate an AdaBoost classifier with the base learner set to be a decision tree of that depth (set `n_estimators=10` and `learning_rate=1`), and then record the 10-fold cross-validated error on the entire breast cancer data set. Plot the resulting curve of accuracy against base classifier depth. Use $101$ as your random state for both the base learner as well as the AdaBoost classifier every time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw2_programming_sol.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "f106a20f20eac506582437e63b5f3e1299dc07823e738d9c61675db8acde4ec9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}