{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import math"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rg2uSiFSRJe"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 1\n",
    "## Dataset Generation"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "oDTSO7-MSRJj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write a function to **generate a training set** of size $m$\n",
    "- randomly generate a weight vector $w \\in \\mathbb{R}^{10}$, normalize length\n",
    "- generate a training set $\\{(x_i , y_i)\\}$ of size m\n",
    "  - $x_i$: random vector in $\\mathbb{R}^{10}$ from $\\textbf{N}(0, I)$\n",
    "  - $y_i$: $\\{0, +1\\}$ with $P[y = +1] = \\sigma(w \\cdot x_i)$ and $P[y = 0] = 1 - \\sigma(w \\cdot x_i)$"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "Tx7KW28cSRJk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def sigmoid(x):\r\n",
    "    return 1/(1+np.exp(-x))\r\n",
    "     \r\n",
    "\r\n",
    "def generate_vector(normalize=False):\r\n",
    "    v = np.random.randn(10)\r\n",
    "    if normalize:\r\n",
    "        sum = 0\r\n",
    "        for i in v:\r\n",
    "            sum += i * i \r\n",
    "        norm = math.sqrt(sum)\r\n",
    "        v = [i*1/norm for i in v]\r\n",
    "    return v\r\n",
    "\r\n",
    "def generate_label(x_i,w):\r\n",
    "    #wT = np.transpose([w])\r\n",
    "    wT = w\r\n",
    "    print(wT)\r\n",
    "    product = np.multiply(wT,x_i)\r\n",
    "    print(product)\r\n",
    "    y_i = sigmoid(product)\r\n",
    "    print('yi' ,y_i)\r\n",
    "    return\r\n",
    "\r\n",
    "def generate_data(m):\r\n",
    "    x = []\r\n",
    "    for i in range(m):\r\n",
    "        x.append(generate_vector(normalize=False))\r\n",
    "    x = np.array(x)\r\n",
    "    return x\r\n",
    "\r\n",
    "generate_label(generate_vector(normalize=True),generate_data(3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[ 1.78441465]\n",
      "  [-1.08265882]\n",
      "  [-0.84700042]]\n",
      "\n",
      " [[ 0.15305618]\n",
      "  [ 2.08582468]\n",
      "  [-0.63189148]]\n",
      "\n",
      " [[ 1.81752022]\n",
      "  [ 0.71504748]\n",
      "  [-2.23193366]]\n",
      "\n",
      " [[ 0.50336481]\n",
      "  [ 0.97367901]\n",
      "  [ 0.46134572]]\n",
      "\n",
      " [[ 0.74163771]\n",
      "  [ 0.86432495]\n",
      "  [-0.0887054 ]]\n",
      "\n",
      " [[ 1.75905748]\n",
      "  [ 0.65647525]\n",
      "  [-0.8702652 ]]\n",
      "\n",
      " [[-2.77813641]\n",
      "  [-0.96664727]\n",
      "  [-0.78778332]]\n",
      "\n",
      " [[-0.68928906]\n",
      "  [ 0.61367299]\n",
      "  [-1.04706238]]\n",
      "\n",
      " [[ 0.04755947]\n",
      "  [-0.47593071]\n",
      "  [ 0.81830973]]\n",
      "\n",
      " [[ 0.67359415]\n",
      "  [-1.50718594]\n",
      "  [-1.0945254 ]]]\n",
      "[[[-1.1523599  -0.37513351  0.22809153 -0.33316487  0.71035883\n",
      "   -0.37971191  0.27828802  0.56825846 -0.45926195  0.54072018]\n",
      "  [ 0.69917192  0.22760495 -0.13839009  0.20214129 -0.43099638\n",
      "    0.2303828  -0.16884583 -0.34477975  0.27864824 -0.32807144]\n",
      "  [ 0.54698571  0.17806301 -0.10826723  0.15814193 -0.33718297\n",
      "    0.18023622 -0.13209378 -0.2697328   0.21799589 -0.25666132]]\n",
      "\n",
      " [[-0.09884239 -0.03217666  0.0195643  -0.02857685  0.06093024\n",
      "   -0.03256937  0.02386985  0.04874174 -0.03939268  0.04637967]\n",
      "  [-1.34700796 -0.43849827  0.26661906 -0.3894406   0.83034736\n",
      "   -0.44385002  0.32529436  0.66424445 -0.53683706  0.63205461]\n",
      "  [ 0.40807018  0.13284114 -0.08077108  0.11797933 -0.25155011\n",
      "    0.13446243 -0.09854651 -0.20122996  0.16263244 -0.19147819]]\n",
      "\n",
      " [[-1.1737392  -0.38209322  0.23232323 -0.33934595  0.72353785\n",
      "   -0.38675656  0.28345099  0.57880114 -0.46778246  0.55075196]\n",
      "  [-0.46177162 -0.15032283  0.09140044 -0.13350524  0.28465373\n",
      "   -0.15215748  0.11151508  0.22771152 -0.18403463  0.21667643]\n",
      "  [ 1.4413639   0.46921443 -0.28529533  0.41672035 -0.88851198\n",
      "    0.47494106 -0.34808075 -0.7107738   0.57444171 -0.67632911]]\n",
      "\n",
      " [[-0.32506874 -0.10582126  0.06434225 -0.09398234  0.20038484\n",
      "   -0.10711278  0.07850216  0.1602998  -0.12955302  0.15253154]\n",
      "  [-0.62879368 -0.20469436  0.12445983 -0.18179387  0.38761254\n",
      "   -0.2071926   0.15184991  0.31007442 -0.25059967  0.29504796]\n",
      "  [-0.29793317 -0.09698768  0.05897119 -0.08613703  0.18365743\n",
      "   -0.09817139  0.07194908  0.14691855 -0.1187384   0.13979875]]\n",
      "\n",
      " [[-0.47894337 -0.15591284  0.09479931 -0.13846985  0.29523906\n",
      "   -0.15781571  0.11566196  0.23617936 -0.19087827  0.22473391]\n",
      "  [-0.55817375 -0.1817051   0.11048172 -0.16137657  0.3440797\n",
      "   -0.18392277  0.13479562  0.27524991 -0.22245478  0.26191107]\n",
      "  [ 0.0572852   0.01864834 -0.0113387   0.01656203 -0.03531279\n",
      "    0.01887594 -0.01383403 -0.02824881  0.02283046 -0.02687985]]\n",
      "\n",
      " [[-1.13598446 -0.36980273  0.22485027 -0.32843048  0.70026438\n",
      "   -0.37431607  0.27433345  0.56018331 -0.45273567  0.53303636]\n",
      "  [-0.42394617 -0.13800933  0.08391348 -0.12256932  0.26133668\n",
      "   -0.13969369  0.10238046  0.20905882 -0.16895967  0.19892765]\n",
      "  [ 0.56200991  0.18295391 -0.11124103  0.16248566 -0.34644446\n",
      "    0.18518681 -0.13572203 -0.27714162  0.22398364 -0.26371111]]\n",
      "\n",
      " [[ 1.79409702  0.58404141 -0.35511331  0.51870089 -1.1059502\n",
      "    0.59116948 -0.43326369 -0.88471562  0.7150201  -0.84184157]\n",
      "  [ 0.62425264  0.2032161  -0.123561    0.18048098 -0.38481326\n",
      "    0.20569629 -0.15075328 -0.30783511  0.24878988 -0.29291717]\n",
      "  [ 0.50874381  0.16561393 -0.10069784  0.14708562 -0.31360919\n",
      "    0.1676352  -0.12285859 -0.25087472  0.20275495 -0.23871713]]\n",
      "\n",
      " [[ 0.44513705  0.1449077  -0.08810788  0.12869593 -0.27439955\n",
      "    0.14667626 -0.10749793 -0.21950859  0.17740509 -0.20887102]\n",
      "  [-0.39630483 -0.1290111   0.07844231 -0.11457779  0.24429749\n",
      "   -0.13058565  0.09570524  0.19542816 -0.15794348  0.18595755]\n",
      "  [ 0.67618404  0.22012159 -0.13384     0.19549515 -0.41682577\n",
      "    0.22280811 -0.1632944  -0.33344383  0.26948664 -0.31728487]]\n",
      "\n",
      " [[-0.0307135  -0.00999832  0.00607925 -0.00887974  0.01893298\n",
      "   -0.01012035  0.00741713  0.01514562 -0.01224057  0.01441165]\n",
      "  [ 0.30735203  0.10005385 -0.0608355   0.08886017 -0.18946358\n",
      "    0.10127498 -0.07422368 -0.15156323  0.12249219 -0.14421835]\n",
      "  [-0.52845751 -0.17203143  0.10459986 -0.15278515  0.32576147\n",
      "   -0.17413102  0.12761933  0.26059606 -0.21061165  0.24796736]]\n",
      "\n",
      " [[-0.43500141 -0.14160819  0.08610169 -0.12576556  0.26815155\n",
      "   -0.14333648  0.10505024  0.21451044 -0.17336562  0.20411509]\n",
      "  [ 0.97332794  0.31685233 -0.19265497  0.28140399 -0.59999667\n",
      "    0.32071943 -0.23505287 -0.47997317  0.38791048 -0.45671327]\n",
      "  [ 0.70683525  0.23009963 -0.13990693  0.20435688 -0.43572035\n",
      "    0.23290793 -0.17069648 -0.34855874  0.28170239 -0.33166729]]]\n",
      "yi [[[0.2400583  0.40730117 0.55677694 0.41747076 0.67048044 0.40619638\n",
      "   0.56912646 0.63836123 0.38716092 0.63197993]\n",
      "  [0.66800415 0.55665686 0.46545759 0.55036395 0.39388843 0.5573423\n",
      "   0.45788854 0.41464889 0.56921479 0.41870995]\n",
      "  [0.63343597 0.5443985  0.4729596  0.53945329 0.41649393 0.54493747\n",
      "   0.46702449 0.43297269 0.55428417 0.43618461]]\n",
      "\n",
      " [[0.4753095  0.49195653 0.50489092 0.49285627 0.51522785 0.49185838\n",
      "   0.50596718 0.51218302 0.4901531  0.51159284]\n",
      "  [0.20635996 0.39209886 0.5662627  0.40385197 0.69642837 0.39082397\n",
      "   0.58061398 0.6602132  0.36892367 0.65295519]\n",
      "  [0.60062505 0.53316153 0.4798182  0.52946067 0.437442   0.53356505\n",
      "   0.47538329 0.44986159 0.54056873 0.45227618]]\n",
      "\n",
      " [[0.23617977 0.40562214 0.55782097 0.41596836 0.6733856  0.40449834\n",
      "   0.57039208 0.6407915  0.38514124 0.63431003]\n",
      "  [0.38656563 0.4624899  0.52283421 0.46667318 0.57068678 0.46203385\n",
      "   0.52784992 0.55668316 0.45412076 0.55395817]\n",
      "  [0.80866577 0.61519781 0.42915604 0.60269819 0.291417   0.61655257\n",
      "   0.41384791 0.32942788 0.63978744 0.3370811 ]]\n",
      "\n",
      " [[0.41944096 0.47356934 0.51608002 0.47652169 0.54992925 0.47324738\n",
      "   0.51961547 0.53998936 0.46765697 0.53805912]\n",
      "  [0.34778412 0.44900434 0.53107485 0.45467629 0.59570783 0.44838636\n",
      "   0.5378897  0.57690343 0.43767591 0.5732315 ]\n",
      "  [0.42606282 0.47577207 0.51473853 0.47847905 0.54578573 0.47547685\n",
      "   0.51797951 0.53666371 0.47035023 0.53489288]]\n",
      "\n",
      " [[0.38250166 0.46110056 0.5236821  0.46543774 0.57327825 0.46062775\n",
      "   0.5288833  0.5587719  0.45242479 0.5559482 ]\n",
      "  [0.36397012 0.4546983  0.52759237 0.45974319 0.58518119 0.45414849\n",
      "   0.53364797 0.56838129 0.44461452 0.56510602]\n",
      "  [0.51431738 0.50466195 0.49716535 0.50414041 0.49117272 0.50471884\n",
      "   0.49654155 0.49293827 0.50570737 0.49328044]]\n",
      "\n",
      " [[0.24305838 0.40858869 0.55597693 0.41862256 0.66824639 0.40749852\n",
      "   0.56815645 0.63649495 0.38871053 0.63019102]\n",
      "  [0.39557285 0.46555233 0.52096607 0.46939597 0.56496485 0.46513326\n",
      "   0.52557278 0.55207518 0.45786028 0.54956856]\n",
      "  [0.63691747 0.54561132 0.47221839 0.54053228 0.41424489 0.54616485\n",
      "   0.46612148 0.43115469 0.55576298 0.43445166]]\n",
      "\n",
      " [[0.85742885 0.6419968  0.41214301 0.62684394 0.24862667 0.64363343\n",
      "   0.39334726 0.29220154 0.67150947 0.30114707]\n",
      "  [0.65118513 0.55062991 0.46914899 0.54499817 0.40496652 0.55124352\n",
      "   0.4623829  0.42364325 0.56187863 0.42728985]\n",
      "  [0.62451195 0.54130911 0.47484679 0.53670525 0.42223402 0.54181093\n",
      "   0.46932393 0.43760821 0.5505158  0.44060252]]\n",
      "\n",
      " [[0.6094824  0.53616367 0.47798727 0.53212965 0.43182733 0.53660346\n",
      "   0.47315137 0.44534215 0.54423532 0.44797126]\n",
      "  [0.40220047 0.46779188 0.51960053 0.47138685 0.56077243 0.4673999\n",
      "   0.52390807 0.54870213 0.46059601 0.54635588]\n",
      "  [0.66288648 0.55480927 0.46658986 0.54871872 0.39727656 0.55547273\n",
      "   0.45926687 0.41740292 0.56696687 0.42133759]]\n",
      "\n",
      " [[0.49232223 0.49750044 0.50151981 0.49778008 0.5047331  0.49746993\n",
      "   0.50185427 0.50378633 0.4969399  0.50360285]\n",
      "  [0.57623879 0.52499262 0.48479581 0.52220044 0.45277529 0.52529713\n",
      "   0.4814526  0.46218156 0.53058482 0.46400777]\n",
      "  [0.37087672 0.4570979  0.52612615 0.46187784 0.58072772 0.45657691\n",
      "   0.5318616  0.56478281 0.44754086 0.56167614]]\n",
      "\n",
      " [[0.39293268 0.46465699 0.52151213 0.46859999 0.56663906 0.46422711\n",
      "   0.52623843 0.55342291 0.45676682 0.55085234]\n",
      "  [0.72578233 0.57855695 0.45198468 0.5698904  0.35434446 0.57949957\n",
      "   0.44150585 0.38225846 0.59577959 0.38776582]\n",
      "  [0.66970149 0.55727244 0.46508021 0.55091216 0.39276119 0.55796519\n",
      "   0.4574292  0.41373197 0.56996354 0.417835  ]]]\n"
     ]
    }
   ],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUHuXrsgSRJl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm 1: logistic regression\n",
    "\n",
    "The goal is to learn $w$.  Algorithm 1 is logistic\n",
    "  regression (you may use the built-in method LogisticRegression for this. Use max_iter=1000)."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "wx2-15fASRJy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vj8b21jgSRJz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm 2: gradient descent with square loss\n",
    "\n",
    "Define square loss as\n",
    "$$L_i(w^{(t)}) = \\frac{1}{2} \\left( \\sigma(w^{(t)} \\cdot x) - y_i \\right)^2$$\n",
    "\n",
    "  Algorithm 2 is\n",
    "  gradient descent with respect to square loss (code this\n",
    "  up yourself -- run for 1000 iterations, use step size eta = 0.01)."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "YzmNdy6ZSRJ3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm 3: stochastic gradient descent with square loss\n",
    "Similar to gradient descent, except we use the gradient at a single random training point every iteration."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "\n",
    "Measure error $\\|w - \\hat{w}\\|_2$ for each method at different sample size. For any\n",
    "  fixed value of $m$, choose many different $w$'s and average the\n",
    "  values $\\|w - \n",
    "  \\hat{w}\\|_2$ for Algorithms 1, 2 and 3.  Plot the results\n",
    "  for for each algorithm as you make $m$ large (use $m=50, 100, 150, 200, 250$).\n",
    "  Also record, for each algorithm, the time taken to run the overall experiment."
   ],
   "metadata": {
    "colab_type": "text",
    "id": "5A-dLi3TSRJ-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from sklearn import datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "cancer = datasets.load_breast_cancer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each depth in $1, \\dots, 5$, instantiate an AdaBoost classifier with the base learner set to be a decision tree of that depth (set `n_estimators=10` and `learning_rate=1`), and then record the 10-fold cross-validated error on the entire breast cancer data set. Plot the resulting curve of accuracy against base classifier depth. Use $101$ as your random state for both the base learner as well as the AdaBoost classifier every time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw2_programming_sol.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "a549b6e126d5f0fb8e948efef61e129ac23a869a935398aeb4639a233ee46c64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}